{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Train, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_smp_train = np.load('data/pars_smp_train.npy')\n",
    "y_smp_train = np.load('data/y_smp_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 15, 1), (1000000, 200, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars_smp_train.shape, y_smp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size = 1000000\n",
    "small_pars_smp_train = pars_smp_train[:test_data_size].copy()\n",
    "small_y_smp_train = y_smp_train[:test_data_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data_size != 1000000:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(small_pars_smp_train, small_y_smp_train, test_size=0.2, shuffle=False, random_state=178)\n",
    "else:\n",
    "   X_train=small_pars_smp_train\n",
    "   y_train=small_y_smp_train\n",
    "   X_valid = np.array(0)\n",
    "   y_valid = np.array(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 15, 1), (1000000, 200, 3), (), ())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(small_y_smp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anty\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    device = 'cuda'\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 quantiles, \n",
    "                 in_shape=1,  \n",
    "                 dropout=0.5):     \n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "        self.num_quantiles = len(quantiles)\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = len(quantiles)\n",
    "        self.dropout = dropout\n",
    "        self.build_model()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def build_model(self): \n",
    "        self.base_model = nn.LSTM(3, 128, 2, batch_first=True)#input in 3 \n",
    "        final_layers = [\n",
    "            nn.Linear(128, 15) for _ in range(len(self.quantiles))#output in 15 \n",
    "        ]\n",
    "        self.final_layers = nn.ModuleList(final_layers)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in chain(self.final_layers):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.base_model(x)\n",
    "        return torch.stack([layer(out[:, -1, :]) for layer in self.final_layers], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantiles):\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "        \n",
    "    def forward(self, preds, target):\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "        losses = []\n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = target - preds[:, i]\n",
    "            losses.append(torch.max((q-1) * errors, q * errors).unsqueeze(1))\n",
    "        loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "class Learner:\n",
    "    def __init__(self, model, optimizer_class, loss_func, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer_class(self.model.parameters())\n",
    "        self.loss_func = loss_func.to(device)\n",
    "        self.device = device\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, x, y, epochs, batch_size):\n",
    "        self.model.train()\n",
    "        for e in tqdm.tqdm(range(epochs)):\n",
    "            shuffle_idx = np.arange(x.shape[0])\n",
    "            np.random.shuffle(shuffle_idx)\n",
    "            x = x[shuffle_idx]\n",
    "            y = y[shuffle_idx]\n",
    "            epoch_losses = []\n",
    "            for idx in range(0, x.shape[0], batch_size):\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_x = torch.from_numpy(\n",
    "                    x[idx : min(idx + batch_size, x.shape[0]),:]\n",
    "                ).float().to(self.device).requires_grad_(False)\n",
    "                #print(batch_x)\n",
    "                #print(type(batch_x))\n",
    "                batch_y = torch.from_numpy(\n",
    "                    y[idx : min(idx + batch_size, y.shape[0])]\n",
    "                ).float().to(self.device).requires_grad_(False)\n",
    "                preds = self.model(batch_x)\n",
    "                loss = self.loss_func(preds, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_losses.append(loss.cpu().detach().numpy())                                \n",
    "            epoch_loss =  np.mean(epoch_losses)\n",
    "            self.loss_history.append(epoch_loss)\n",
    "            print(\"Epoch {}: {}\".format(e+1, epoch_loss))\n",
    "                \n",
    "    def predict(self, x, mc=False):\n",
    "        if mc:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        return self.model(torch.from_numpy(x).float().to(self.device).requires_grad_(False)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "quantiles = [.1, .25, .50,.75,.90]\n",
    "model = q_model(quantiles, dropout=0.1)\n",
    "loss_func = QuantileLoss(quantiles)\n",
    "learner = Learner(model, partial(torch.optim.Adam, weight_decay=1e-6), loss_func, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 15, 1) (1000000, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 15)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.squeeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:17<29:38, 197.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.2375107705593109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:22<25:23, 190.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 0.18350750207901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [09:25<21:48, 186.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 0.17152968049049377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [12:28<18:32, 185.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 0.16355277597904205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [15:32<15:23, 184.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 0.15938754379749298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [18:35<12:17, 184.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 0.15694768726825714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [21:38<09:11, 183.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 0.1549845039844513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [24:41<06:07, 183.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 0.15358981490135193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [27:44<03:03, 183.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 0.1526651829481125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [30:47<00:00, 184.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 0.15180358290672302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "epochs = 10\n",
    "learner.fit(y_train, X_train.squeeze(2), epochs, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 200, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Learner at 0x16249e79c40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"learner.class\"\n",
    "file = open(filename, 'wb') \n",
    "pickle.dump(learner, file=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Hahaton\\noa_cb\\playbook-quantiles_model.ipynb Ячейка 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook-quantiles_model.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tmp\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the meshed x-axis\n",
    "tmp = learner.predict(xx)\n",
    "y_lower, y_pred, y_upper = tmp[:, 0], tmp[:, 2], tmp[:, 4]\n",
    "\n",
    "# Plot the function, the prediction and the 90% confidence interval based on\n",
    "# the MSE\n",
    "fig = plt.figure()\n",
    "plt.plot(xx, f(xx), 'g:', label=u'$f(x) = x\\,\\sin(x)$')\n",
    "plt.plot(X, y, 'b.', markersize=10, label=u'Observations')\n",
    "plt.plot(xx, y_pred, 'r-', label=u'Prediction')\n",
    "plt.plot(xx, y_upper, 'k-')\n",
    "plt.plot(xx, y_lower, 'k-')\n",
    "plt.fill(np.concatenate([xx, xx[::-1]]),\n",
    "         np.concatenate([y_upper, y_lower[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='90% prediction interval')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.ylim(-10, 20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = learner.predict(X)\n",
    "np.mean(predictions[:, 0]), np.mean(predictions[:, 2]), np.mean(predictions[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_the_range = np.sum((y >= predictions[:, 0]) & (y <= predictions[:, 4]))\n",
    "print(\"Percentage in the range (expecting 90%):\", in_the_range / len(y) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_the_range = np.sum((y < predictions[:, 0]) | (y > predictions[:, 4]))\n",
    "print(\"Percentage out of the range (expecting 10%):\", out_of_the_range / len(y)  * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroEconomicModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MacroEconomicModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Используйте только последний временной шаг для предсказания\n",
    "        return out\n",
    "\n",
    "# Определение размеров входных и выходных данных\n",
    "input_size = 3  # Размерность данных о приросте ВВП, инфляции и процентной ставке\n",
    "hidden_size = 64  # Размер скрытого состояния RNN\n",
    "output_size = 15  # Размер параметров модели\n",
    "num_layers = 2  # Количество слоев LSTM     \n",
    "                                \n",
    "# Создание экземпляра модели\n",
    "model = MacroEconomicModel(input_size, hidden_size, output_size, num_layers).to(device=device)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN BiLSTM\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional, cnn_out_channels, cnn_kernel_size):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        # 1D Convolutional Layer\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, cnn_out_channels, kernel_size=cnn_kernel_size, stride=1),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # BiLSTM Layer\n",
    "        self.bilstm = nn.LSTM(input_size=cnn_out_channels, \n",
    "                              hidden_size=hidden_dim, \n",
    "                              num_layers=num_layers, \n",
    "                              bidirectional=bidirectional, \n",
    "                              batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, sequence_length, input_dim)\n",
    "        x = x.permute(0, 2, 1)  # Reshape for Conv1d: (batch_size, input_dim, sequence_length)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # Reshape for BiLSTM: (batch_size, sequence_length, cnn_out_channels)\n",
    "        output, (hidden, cell) = self.bilstm(x)\n",
    "        # Take the output of the last time step\n",
    "        if self.bilstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the model with your specific parameters\n",
    "input_dim = 3  # Input dimension\n",
    "hidden_dim = 64  # Hidden dimension for BiLSTM\n",
    "output_dim = 15  # Output dimension\n",
    "num_layers = 2  # Number of BiLSTM layers\n",
    "bidirectional = True  # Use bidirectional BiLSTM\n",
    "cnn_out_channels = 64  # Number of CNN output channels\n",
    "cnn_kernel_size = 3  # Kernel size for CNN\n",
    "\n",
    "model = CNN_BiLSTM(input_dim, hidden_dim, output_dim, num_layers, bidirectional, cnn_out_channels, cnn_kernel_size).to(device=device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры PyTorch\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_train = torch.Tensor(X_train)\n",
    "\n",
    "y_valid = torch.Tensor(y_valid)\n",
    "X_valid = torch.Tensor(X_valid)\n",
    "\n",
    "\n",
    "batch_size = 180\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "train_dataset = TensorDataset(y_train, X_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "valid_dataset = TensorDataset(y_valid, X_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "sss = int(X_train.shape[0]/batch_size+1)\n",
    "# Обучение модели\n",
    "num_epochs = 10  # Количество эпох обучения\n",
    "history = {\n",
    "    'training_loss':[],\n",
    "    'validation_loss':[]\n",
    "}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_losses = []#.to(device)\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Проход вперед (forward pass)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Вычисление функции потерь\n",
    "        loss = torch.sqrt(criterion(outputs, targets.squeeze()))\n",
    "        #batch_losses.append(loss.detach().numpy())\n",
    "        batch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(len(batch_losses))\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_val_losses = []\n",
    "        for batch_val in valid_loader:\n",
    "            inputs_val, targets_val = batch_val\n",
    "            inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "            #print(inputs_val.shape, targets_val.shape)\n",
    "            model.eval()\n",
    "\n",
    "            # Проход вперед (forward pass)\n",
    "            outputs_val = model(inputs_val)\n",
    "\n",
    "            # Вычисление функции потерь\n",
    "            loss_val = torch.sqrt(criterion(outputs_val, targets_val.squeeze()))\n",
    "            batch_val_losses.append(loss_val)\n",
    "            #print(batch_val_losses)\n",
    "           # print(type(batch_val_losses))\n",
    "            validation_loss = torch.mean(torch.stack(batch_val_losses))\n",
    "\n",
    "\n",
    "\n",
    "    # Выводим информацию о процессе обучения\n",
    "    #print(f'Эпоха [{epoch + 1}/{num_epochs}], Потери train: {loss.item()}')\n",
    "    print(f'Эпоха [{epoch + 1}/{num_epochs}], Потери train: {training_loss.item()}, Loss valid: {validation_loss.item()}')\n",
    "    history['training_loss'].append(training_loss.item())\n",
    "    history['validation_loss'].append(validation_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)\n",
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(torch.stack(history['training_loss']).cpu())\n",
    "plt.plot(history['training_loss'])\n",
    "plt.plot(history['validation_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_CNN_BiLSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_smp_test = np.load('data/y_smp_test.npy')\n",
    "y_test = torch.Tensor(y_smp_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "test_dataset = TensorDataset(y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalute model on test data\n",
    "with torch.no_grad():\n",
    "    all_outputs_test = []\n",
    "    for batch in test_loader:\n",
    "        inputs_test = batch[0]\n",
    "        inputs_test = inputs_test.to(device=device)\n",
    "        model.eval()\n",
    "\n",
    "        outputs_test = model(inputs_test).unsqueeze(2)\n",
    "        all_outputs_test.append(outputs_test)\n",
    "\n",
    "    # Concat to common tensor\n",
    "    final_outputs_test = torch.cat(all_outputs_test, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Window size\n",
    "\n",
    "# Empty tensor for saving result\n",
    "result = torch.zeros((final_outputs_test.size(0), final_outputs_test.size(1), 6))\n",
    "\n",
    "for i in range(0, final_outputs_test.size(0), batch_size):\n",
    "    batch = final_outputs_test[i:i+batch_size]\n",
    "    \n",
    "    # Calculate mean\n",
    "    batch_mean = torch.mean(batch, dim=0).squeeze(1)\n",
    "\n",
    "    # Sort batch for calculate quantiles\n",
    "    sorted_batch, _ = torch.sort(batch, dim=0)\n",
    "    sorted_batch = sorted_batch.to(device=device)\n",
    "    quantiles = torch.quantile(sorted_batch, torch.Tensor([0.1, 0.25, 0.5, 0.75, 0.9]).to(device=device), dim=0).to(device=device)\n",
    "\n",
    "    # Save to final tensor\n",
    "    result[i:i+batch_size, :, 0] = batch_mean\n",
    "    result[i:i+batch_size, :, 1:6] = quantiles\n",
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Window size\n",
    "\n",
    "# Empty tensor for saving result\n",
    "result = torch.zeros((final_outputs_test.size(0), final_outputs_test.size(1), 6))\n",
    "\n",
    "for i in range(0, final_outputs_test.size(0), batch_size):\n",
    "    batch = final_outputs_test[i:i+batch_size]\n",
    "    \n",
    "    # Calculate mean\n",
    "    batch_mean = torch.mean(batch, dim=0).squeeze(1)\n",
    "\n",
    "    # Sort batch for calculate quantiles\n",
    "    sorted_batch, _ = torch.sort(batch, dim=0)\n",
    "    sorted_batch = sorted_batch.to(device=device)\n",
    "    quantiles = torch.quantile(sorted_batch, torch.Tensor([0.1, 0.25, 0.5, 0.75, 0.9]).to(device=device), dim=0)\n",
    "\n",
    "    # Save to final tensor\n",
    "    result[i:i+batch_size, :, 0] = batch_mean\n",
    "    result[i:i+batch_size, :, 1:6] = quantiles\n",
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file='submission.npy', arr=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

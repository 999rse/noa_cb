{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Train, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_smp_train = np.load('data/pars_smp_train.npy')\n",
    "y_smp_train = np.load('data/y_smp_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 15, 1), (1000000, 200, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars_smp_train.shape, y_smp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_pars_smp_train = pars_smp_train[:1000].copy()\n",
    "small_y_smp_train = y_smp_train[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(small_pars_smp_train, small_y_smp_train, test_size=0.2, shuffle=False, random_state=178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 15, 1), (800, 200, 3), (200, 15, 1), (200, 200, 3))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#plt.plot(torch.stack(history['training_loss']).cpu())\n",
    "def print_history(history):\n",
    "    plt.plot(history['training_loss'])\n",
    "    plt.plot(history['validation_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, name='trained_model2_dev'):\n",
    "    torch.save(model.state_dict(), name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroEconomicModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(MacroEconomicModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Используйте только последний временной шаг для предсказания\n",
    "        return out\n",
    "    \n",
    "def create_n_train_model(X_train, X_valid, y_train, y_valid, loss_function=nn.MSELoss(), loss_patch=None):\n",
    "\n",
    "\n",
    "# Определение размеров входных и выходных данных\n",
    "    input_size = 3  # Размерность данных о приросте ВВП, инфляции и процентной ставке\n",
    "    hidden_size = 64  # Размер скрытого состояния RNN\n",
    "    output_size = 15  # Размер параметров модели\n",
    "    num_layers = 2  # Количество слоев LSTM     \n",
    "                                \n",
    "# Создание экземпляра модели\n",
    "    model = MacroEconomicModel(input_size, hidden_size, output_size, num_layers).to(device=device)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "    criterion = loss_function\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Преобразование данных в тензоры PyTorch\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_train = torch.Tensor(X_train)\n",
    "\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "\n",
    "\n",
    "    batch_size = 180\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "    train_dataset = TensorDataset(y_train, X_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    valid_dataset = TensorDataset(y_valid, X_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    sss = int(X_train.shape[0]/batch_size+1)\n",
    "    # Обучение модели\n",
    "    num_epochs = 10  # Количество эпох обучения\n",
    "    history = {\n",
    "        'training_loss':[],\n",
    "        'validation_loss':[]\n",
    "    }\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_losses = []#.to(device)\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Обнуляем градиенты\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Проход вперед (forward pass)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Вычисление функции потерь\n",
    "            loss = torch.sqrt(criterion(outputs, targets.squeeze()))\n",
    "            #batch_losses.append(loss.detach().numpy())\n",
    "            batch_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #print(len(batch_losses))\n",
    "        training_loss = np.mean(batch_losses)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_val_losses = []\n",
    "            for batch_val in valid_loader:\n",
    "                inputs_val, targets_val = batch_val\n",
    "                inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "                #print(inputs_val.shape, targets_val.shape)\n",
    "                model.eval()\n",
    "\n",
    "                # Проход вперед (forward pass)\n",
    "                outputs_val = model(inputs_val)\n",
    "\n",
    "                # Вычисление функции потерь\n",
    "                loss_val = torch.sqrt(criterion(outputs_val, targets_val.squeeze()))\n",
    "                batch_val_losses.append(loss_val)\n",
    "                #print(batch_val_losses)\n",
    "            # print(type(batch_val_losses))\n",
    "                validation_loss = torch.mean(torch.stack(batch_val_losses))\n",
    "\n",
    "\n",
    "\n",
    "        # Выводим информацию о процессе обучения\n",
    "        #print(f'Эпоха [{epoch + 1}/{num_epochs}], Потери train: {loss.item()}')\n",
    "        print(f'Эпоха [{epoch + 1}/{num_epochs}], Потери train: {training_loss.item()}, Loss valid: {validation_loss.item()}')\n",
    "        history['training_loss'].append(training_loss.item())\n",
    "        history['validation_loss'].append(validation_loss.item())\n",
    "    print_history(history)\n",
    "    save_model(model, name='model_'+str(criterion))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_patch(criterion):\n",
    "    return torch.sqrt(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not HuberLoss",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Hahaton\\noa_cb\\playbook.ipynb Ячейка 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m create_n_train_model(X_train, X_valid, y_train, y_valid, loss_function\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mHuberLoss)\n",
      "\u001b[1;32mc:\\Hahaton\\noa_cb\\playbook.ipynb Ячейка 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Вычисление функции потерь\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msqrt(criterion(outputs, targets\u001b[39m.\u001b[39;49msqueeze()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m#batch_losses.append(loss.detach().numpy())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y111sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m batch_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not HuberLoss"
     ]
    }
   ],
   "source": [
    "model = create_n_train_model(X_train, X_valid, y_train, y_valid, loss_function=nn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MacroEconomicModel(\n",
       "  (rnn): LSTM(3, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 15])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 200, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_smp_test = np.load('data/y_smp_test.npy')\n",
    "y_test = torch.Tensor(y_smp_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "test_dataset = TensorDataset(y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Hahaton\\noa_cb\\playbook.ipynb Ячейка 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y120sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     inputs_test \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y120sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     inputs_test \u001b[39m=\u001b[39m inputs_test\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y120sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hahaton/noa_cb/playbook.ipynb#Y120sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs_test \u001b[39m=\u001b[39m model(inputs_test)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evalute model on test data\n",
    "with torch.no_grad():\n",
    "    all_outputs_test = []\n",
    "    for batch in test_loader:\n",
    "        inputs_test = batch[0]\n",
    "        inputs_test = inputs_test.to(device=device)\n",
    "        model.eval()\n",
    "\n",
    "        outputs_test = model(inputs_test).unsqueeze(2)\n",
    "        all_outputs_test.append(outputs_test)\n",
    "\n",
    "    # Concat to common tensor\n",
    "    final_outputs_test = torch.cat(all_outputs_test, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 15, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outputs_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Window size\n",
    "\n",
    "# Empty tensor for saving result\n",
    "result = torch.zeros((final_outputs_test.size(0), final_outputs_test.size(1), 6))\n",
    "\n",
    "for i in range(0, final_outputs_test.size(0), batch_size):\n",
    "    batch = final_outputs_test[i:i+batch_size]\n",
    "    \n",
    "    # Calculate mean\n",
    "    batch_mean = torch.mean(batch, dim=0).squeeze(1)\n",
    "\n",
    "    # Sort batch for calculate quantiles\n",
    "    sorted_batch, _ = torch.sort(batch, dim=0)\n",
    "\n",
    "    quantiles = torch.quantile(sorted_batch, torch.Tensor([0.1, 0.25, 0.5, 0.75, 0.9]), dim=0).to(device=device)\n",
    "\n",
    "    # Save to final tensor\n",
    "    result[i:i+batch_size, :, 0] = batch_mean\n",
    "    result[i:i+batch_size, :, 1:6] = quantiles\n",
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5  # Window size\n",
    "\n",
    "# Empty tensor for saving result\n",
    "result = torch.zeros((final_outputs_test.size(0), final_outputs_test.size(1), 6))\n",
    "\n",
    "for i in range(0, final_outputs_test.size(0), batch_size):\n",
    "    batch = final_outputs_test[i:i+batch_size]\n",
    "    \n",
    "    # Calculate mean\n",
    "    batch_mean = torch.mean(batch, dim=0).squeeze(1)\n",
    "\n",
    "    # Sort batch for calculate quantiles\n",
    "    sorted_batch, _ = torch.sort(batch, dim=0)\n",
    "    sorted_batch = sorted_batch.to(device=device)\n",
    "    quantiles = torch.quantile(sorted_batch, torch.Tensor([0.1, 0.25, 0.5, 0.75, 0.9]).to(device=device), dim=0)\n",
    "\n",
    "    # Save to final tensor\n",
    "    result[i:i+batch_size, :, 0] = batch_mean\n",
    "    result[i:i+batch_size, :, 1:6] = quantiles\n",
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file='submission.npy', arr=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
